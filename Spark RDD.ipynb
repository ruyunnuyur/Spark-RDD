{"cells":[{"cell_type":"markdown","source":["# Introduction to Spark programming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"231602cf-d928-4fb8-b56c-30b4148029ee"}}},{"cell_type":"markdown","source":["# Word Count\n\nThe \"Hello World!\" of distributed programming is the wordcount. Basically, you want to count easily number of different words contained in an unstructured text. You will write some code to perform this task on the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) retrieved from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page).\n\n[Spark's Python API reference](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) could provide some help\n\n### ** Part 1: Creating a base RDD and pair RDDs **\n\n#### In this part of the lab, we will explore creating a base RDD with `parallelize` and using pair RDDs to count words.\n\n#### We'll start by generating a base RDD by using a Python list and the `sc.parallelize` method.  Then we'll print out the type of the base RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7768cd1d-dc3f-44c5-9042-374e975f753b"}}},{"cell_type":"code","source":["#Please, run this first\nimport hashlib\nimport sys\n\ndef hash(x):\n  return hashlib.sha1(str(x).encode('utf-8')).hexdigest()\n\nassert sys.version_info.major == 3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e056337-c7c3-4118-92e9-f8091e3be2cf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["words_list = ['we', 'few', 'we', 'happy', 'few', \"we\", \"band\", \"of\", \"brothers\"]\nwords_RDD = sc.parallelize(words_list, 4)\n\n# Print the type of words_RDD\nprint(type(words_RDD))"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c350e61d-970b-4471-88f7-7910777ee718"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<class 'pyspark.rdd.RDD'>\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<class 'pyspark.rdd.RDD'>\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We want to capitalize each word contained in a RDD. For such transformation, we use a `map`, as we want to transform a RDD of **n** elements into another RDD of **n** using a function that gets and returns one single element.\n\nPlease implement `capitalize`function in the cell below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09db613c-c36a-4a47-8dd4-6dcfd2e0beb0"}}},{"cell_type":"code","source":["def capitalize(word):\n    \"\"\"Capitalize lowercase `words`.\n\n    Args:\n        word (str): A lowercase string.\n\n    Returns:\n        str: A string which first letter is uppercase.\n    \"\"\"\n    return word.capitalize()\n\nprint(capitalize('we'))"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95ec13f3-1952-46b0-a8f8-2d204148c75f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"We\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["We\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Apply `capitalize` to the base RDD, using a [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) transformation that applies the `capitalize()` function to each element. Then call the [collect()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) action to retrieve the values of the transformed RDD, and print them."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6494b5e-5674-4ae0-b72c-952d5b5843e1"}}},{"cell_type":"code","source":["capital_RDD = words_RDD.map(capitalize)\nlocal_result = capital_RDD.collect()\n\nprint(local_result)\n\nassert hash(local_result) == 'bd73c54004cc9655159aceb703bc14fe93369fb1',\\\n       'Capitalize'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"902a9123-31ed-4a03-8a87-5d7b2a366c14"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['We', 'Few', 'We', 'Happy', 'Few', 'We', 'Band', 'Of', 'Brothers']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['We', 'Few', 'We', 'Happy', 'Few', 'We', 'Band', 'Of', 'Brothers']\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Do the same using a lambda function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e4c5fa3-6252-4a2d-aa57-b86a42e04e29"}}},{"cell_type":"code","source":["capital_lambda_RDD = words_RDD.map(lambda word: capitalize(word))\nlocal_result = capital_lambda_RDD.collect()\n\nprint(local_result)\n\nassert hash(local_result) == 'bd73c54004cc9655159aceb703bc14fe93369fb1',\\\n       'Capitalize with lambda'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7bf294bd-e63c-415b-9142-2e7660773a23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['We', 'Few', 'We', 'Happy', 'Few', 'We', 'Band', 'Of', 'Brothers']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['We', 'Few', 'We', 'Happy', 'Few', 'We', 'Band', 'Of', 'Brothers']\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Now use `map()` and a `lambda` function to return the number of characters in each word, and `collect` this result directly into a variable."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52ad23df-5959-4359-b93e-9243adbe6bf7"}}},{"cell_type":"code","source":["plural_lengths = (capital_RDD.map(lambda word: len(word)).collect())\n\nprint(plural_lengths)\n\nassert hash(plural_lengths) == '0772853c8e180c1bed8cfe9bde35aae79b277381',\\\n       'incorrect values for plural_lengths'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e80a7623-2138-42f2-abe5-458fc09a43f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[2, 3, 2, 5, 3, 2, 4, 2, 8]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[2, 3, 2, 5, 3, 2, 4, 2, 8]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["To program a wordcount, we will need `pair RDD` objects. A pair RDD is an RDD where each element is a pair tuple `(k, v)` where `k` is the key and `v` is the value. In this example, we will create a pair consisting of `('<word>', 1)` for each word element in the RDD.\n\nCreate the pair RDD using the `map()` transformation with a `lambda()` on `words_RDD`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffdbb5f6-b831-4866-8cd0-fedd3c935561"}}},{"cell_type":"code","source":["words_pair_RDD = words_RDD.map(lambda x: (x, 1))\n\nprint(words_pair_RDD.collect())\n\nassert hash(words_pair_RDD.collect()) == 'fb67a530034e01395386569ef29bf5565b503ec6', \\\n       'incorrect value for wrods_pair_RDD'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"315b62ae-f719-4bf2-b174-a6cb569cfa13"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('we', 1), ('few', 1), ('we', 1), ('happy', 1), ('few', 1), ('we', 1), ('band', 1), ('of', 1), ('brothers', 1)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('we', 1), ('few', 1), ('we', 1), ('happy', 1), ('few', 1), ('we', 1), ('band', 1), ('of', 1), ('brothers', 1)]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Now, let's count the number of times a particular word appears in the RDD. There are multiple ways to perform the counting, but some are much less efficient or scalable than others.\n\nA naive approach would be to `collect()` all of the elements and count them in the driver program. While this approach could work for small datasets, it is not scalable as the result of `collect()` would have to fit in the driver's memory. When you should use `collect()` with care, always asking yourself what is the size of data you want to retrieve.\n\nIn order to program a scalable wordcount, you will need to use parallel operations.\n\n#### `groupByKey()` approach\n\nAn approach you might first consider is based on using the [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) transformation. This transformation groups all the elements of the RDD with the same key into a single list, stored in one of the partitions. \n \nUse `groupByKey()` on `words_pair_RDD`\n to generate a pair RDD of type `('word', list)`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81dd34c6-9092-4ca7-b145-3cd7a3ea5c2e"}}},{"cell_type":"code","source":["words_grouped = words_pair_RDD.groupByKey()\n\nfor key, value in words_grouped.collect():\n    print('{0}: {1}'.format(key, list(value)))\n    \nassert hash(sorted(words_grouped.mapValues(lambda x: list(x)).collect())) == 'fdaad77fd81ef2df23d98ff7fd438fa700ca1fcf',\\\n       'incorrect value for words_grouped'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3abf471-3033-4a78-b5fe-d615d8e26b81"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"of: [1]\nfew: [1, 1]\nbrothers: [1]\nwe: [1, 1, 1]\nband: [1]\nhappy: [1]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["of: [1]\nfew: [1, 1]\nbrothers: [1]\nwe: [1, 1, 1]\nband: [1]\nhappy: [1]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Using the `groupByKey()` transformation results in an `pairRDD` containing words as keys, and Python iterators as values. Python iterators are a class of objects on which we can iterate, i.e.\n\n    a = some_iterator()\n    for elem in a:\n        # do stuff with elem\n\nPython lists and dictionnaries are iterators for example.\n\nNow sum the iterator using a `map()` transformation. The result should be a pair RDD consisting of (word, count) pairs.\n\nHint: there exists a `sum` function\nHint 2: you want to perform an operation only on the values of the pairRDD. Take a look at [mapValues()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapValues)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad8b938c-e993-4cde-b657-1bee1336ac1e"}}},{"cell_type":"code","source":["word_grouped_counts = words_grouped.mapValues(sum)\n\nprint(word_grouped_counts.collect())\n\nassert hash(sorted(word_grouped_counts.collect())) == 'c20f05d36e98ae399b2cbe5b6cb9bf01b675455a',\\\n       'incorrect value for word_grouped_counts'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9837242-54c9-419e-b242-bbc644714283"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["There are two problems with using `groupByKey()`:\n  + The operation requires a lot of data movement to move all the values into the appropriate partitions (remember the cost of network communications!).\n  + The lists can be very large. Consider a word count of English Wikipedia: the lists for common words (e.g., the, a, etc.) would be huge and could exhaust the available memory of a worker.\n\nA better approach is to start from the pair RDD and then use the [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) transformation to create a new pair RDD. The `reduceByKey()` transformation gathers together pairs that have the same key and applies the function provided to two values at a time, iteratively reducing all of the values to a single value. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions, allowing it to scale efficiently to large datasets.\n\nCompute the word count using `reduceByKey`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"260ed723-00db-4d90-b939-63458034a3e0"}}},{"cell_type":"code","source":["word_counts = words_pair_RDD.reduceByKey(lambda x,y: x+y)\n \nprint(word_counts.collect())\n\nassert hash(sorted(word_counts.collect())) == 'c20f05d36e98ae399b2cbe5b6cb9bf01b675455a',\\\n       'incorrect value for word_counts'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d5e63c3-fdf9-470e-a839-519ed230d560"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["You should be able to perform the word count by composing functions, resulting in a smaller code. Use the `map()` on word RDD to create a pair RDD, apply the `reduceByKey()` transformation, and `collect` in one statement."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07297084-c89c-4c1d-848e-890f733d948a"}}},{"cell_type":"code","source":["from operator import add\nword_counts_collected = (words_RDD\n                         .map(lambda x: (x, 1))\n                         .reduceByKey(add)\n                         .collect()\n                        )\n\nprint(word_counts_collected)\n\nassert hash(sorted(word_counts_collected)) == 'c20f05d36e98ae399b2cbe5b6cb9bf01b675455a', \\\n       'incorrect value for word_counts_collected'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1b06eda-dccd-4c75-b55f-3675b74a37e3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Compute the number of unique words using one of the RDD you have already created."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6eb432c-50dc-4641-9d8c-e50b430179fa"}}},{"cell_type":"code","source":["unique_words = len(words_RDD.distinct().collect())\n\nprint(unique_words)\n\nassert unique_words == 6, 'incorrect count of unique_words'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b957a918-390c-46aa-8e2f-4a851d57fad4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"6\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["6\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Use a `reduce()` action to sum the counts in `wordCounts` and then divide by the number of unique words to find the mean number of words per unique word in `word_counts`.  First `map()` RDD `word_counts`, which consists of (key, value) pairs, to an RDD of values."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31abf4b3-2c36-42a2-a2a7-294e3f144391"}}},{"cell_type":"code","source":["from operator import add\ntotal_count = (word_counts\n              .map(lambda x: x[1])\n              .reduce(add))\n\naverage = total_count / float(unique_words)\n\nprint(total_count)\nprint(round(average, 2))\n\nassert round(average, 2) == 1.5, 'incorrect value of average'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9661002-7f3f-4137-a8b1-07b2b9bca031"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"9\n1.5\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["9\n1.5\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Part 2: Apply word count to a file\n\nIn this section we will finish developing our word count application.  We'll have to build the `word_count` function, deal with real world problems like capitalization and punctuation, load in our data source, and compute the word count on the new data.\n\nFirst, define a function for word counting. You should reuse the techniques that have been covered in earlier parts of this lab.  This function should take in an RDD that is a list of words like `words_RDD` and return a pair RDD that has all of the words and their associated counts."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82f197e0-4e40-4427-9d5c-6e0b394b250b"}}},{"cell_type":"code","source":["def word_count(word_list_RDD):\n    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n\n    Args:\n        wordListRDD (RDD of str): An RDD consisting of words.\n\n    Returns:\n        RDD of (str, int): An RDD consisting of (word, count) tuples.\n    \"\"\"\n    word_counts_collected = (word_list_RDD\n                             .map(lambda x: (x, 1))\n                             .reduceByKey(add))\n    \n    return word_counts_collected\n\nprint(word_count(words_RDD).collect())\n\nassert hash(sorted(word_count(words_RDD).collect())) == 'c20f05d36e98ae399b2cbe5b6cb9bf01b675455a', \\\n       'incorrect definition for word_count function'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb8cb83e-16f9-46a6-be29-e72eaa180f58"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Real world data is more complicated than the data we have been using in this lab. Some of the issues we have to address are:\n  + Words should be counted independent of their capitialization (e.g., Spark and spark should be counted as the same word).\n  + All punctuation should be removed.\n  + Any leading or trailing spaces on a line should be removed.\n \nDefine the function `removePunctuation` that converts all text to lower case, removes any punctuation, and removes leading and trailing spaces.  Use the Python [re](https://docs.python.org/2/library/re.html) module to remove any text that is not a letter, number, or space. Reading `help(re.sub)` might be useful.\n\nIf you have never used regex (regular expressions) before, you can refer to [Regular-expressions.info](http://www.regular-expressions.info/python.html)\n\nIn order to test your regular expressions, you can use [Regex Tester](http://www.regexpal.com)\n\nRegex can be a bit obscure at beginning, don't hesitate to search in [StackOverflow](http://stackoverflow.com) or to ask me for some help."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50001e1f-54b2-479f-9c16-98e52393faf1"}}},{"cell_type":"code","source":["import re\nhelp(re.sub)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58ed05b0-1de1-4957-9c35-006dc6b496f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Help on function sub in module re:\n\nsub(pattern, repl, string, count=0, flags=0)\n    Return the string obtained by replacing the leftmost\n    non-overlapping occurrences of the pattern in string by the\n    replacement repl.  repl can be either a string or a callable;\n    if a string, backslash escapes in it are processed.  If it is\n    a callable, it's passed the Match object and must return\n    a replacement string to be used.\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Help on function sub in module re:\n\nsub(pattern, repl, string, count=0, flags=0)\n    Return the string obtained by replacing the leftmost\n    non-overlapping occurrences of the pattern in string by the\n    replacement repl.  repl can be either a string or a callable;\n    if a string, backslash escapes in it are processed.  If it is\n    a callable, it's passed the Match object and must return\n    a replacement string to be used.\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["import re\nimport string\n\n# Hint: string.punctuation contains all the punctuation symbols\n\ndef remove_punctuation(text):\n    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n\n    Note:\n        Only spaces, letters, and numbers should be retained.  Other characters should should be\n        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n        punctuation is removed.\n\n    Args:\n        text (str): A string.\n\n    Returns:\n        str: The cleaned up string.\n    \"\"\"\n    lowertext = text.lower()\n    clean_text = re.sub(r'[^\\w\\s]','',lowertext).strip()\n    return clean_text\n\nprint(remove_punctuation('Hello World!'))\nprint(remove_punctuation(' No under_score!'))\n\nassert remove_punctuation(\"  Remove punctuation: there ARE trailing spaces. \") == 'remove punctuation there are trailing spaces', \\\n       'incorrect definition for remove_punctuation function'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2471a595-7446-4024-ad44-7a94b1dbb580"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"hello world\nno under_score\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["hello world\nno under_score\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["For the next part of this lab, we will use the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page). To convert a text file into an RDD, we use the `SparkContext.textFile()` method. We also apply the recently defined `remove_punctuation()` function using a `map()` transformation to strip out the punctuation and change all text to lowercase.  Since the file is large we use `take(15)`, instead of `collect()` so that we only print 15 lines.\n\nTake a look at [zipWithIndex()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.zipWithIndex) and [take()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) to understand the print statement.\n\nTo understand how we load the data, look at [databricks documentation](https://docs.databricks.com/user-guide/importing-data.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffb95cb4-16a9-425d-9a1b-8e4707628d57"}}},{"cell_type":"code","source":["%sh wget https://waterponey.github.io/BigDataClass/files/shakespeare.txt -O shakespeare.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bde285e7-9e23-4c93-bb0a-2b46f3061eb1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"--2022-01-16 20:28:13--  https://waterponey.github.io/BigDataClass/files/shakespeare.txt\nResolving waterponey.github.io (waterponey.github.io)... 185.199.111.153, 185.199.108.153, 185.199.109.153, ...\nConnecting to waterponey.github.io (waterponey.github.io)|185.199.111.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5589889 (5.3M) [text/plain]\nSaving to: ‘shakespeare.txt’\n\n     0K .......... .......... .......... .......... ..........  0% 5.03M 1s\n    50K .......... .......... .......... .......... ..........  1% 5.16M 1s\n   100K .......... .......... .......... .......... ..........  2% 5.45M 1s\n   150K .......... .......... .......... .......... ..........  3% 17.3M 1s\n   200K .......... .......... .......... .......... ..........  4% 26.9M 1s\n   250K .......... .......... .......... .......... ..........  5% 11.2M 1s\n   300K .......... .......... .......... .......... ..........  6% 64.5M 1s\n   350K .......... .......... .......... .......... ..........  7% 18.6M 1s\n   400K .......... .......... .......... .......... ..........  8% 60.1M 0s\n   450K .......... .......... .......... .......... ..........  9% 51.5M 0s\n   500K .......... .......... .......... .......... .......... 10% 10.2M 0s\n   550K .......... .......... .......... .......... .......... 10% 73.6M 0s\n   600K .......... .......... .......... .......... .......... 11%  153M 0s\n   650K .......... .......... .......... .......... .......... 12% 31.2M 0s\n   700K .......... .......... .......... .......... .......... 13% 35.1M 0s\n   750K .......... .......... .......... .......... .......... 14% 5.35M 0s\n   800K .......... .......... .......... .......... .......... 15% 15.0M 0s\n   850K .......... .......... .......... .......... .......... 16%  138M 0s\n   900K .......... .......... .......... .......... .......... 17% 18.8M 0s\n   950K .......... .......... .......... .......... .......... 18% 41.6M 0s\n  1000K .......... .......... .......... .......... .......... 19%  130M 0s\n  1050K .......... .......... .......... .......... .......... 20%  145M 0s\n  1100K .......... .......... .......... .......... .......... 21% 10.1M 0s\n  1150K .......... .......... .......... .......... .......... 21% 30.9M 0s\n  1200K .......... .......... .......... .......... .......... 22% 15.9M 0s\n  1250K .......... .......... .......... .......... .......... 23% 23.3M 0s\n  1300K .......... .......... .......... .......... .......... 24%  132M 0s\n  1350K .......... .......... .......... .......... .......... 25% 18.0M 0s\n  1400K .......... .......... .......... .......... .......... 26% 32.1M 0s\n  1450K .......... .......... .......... .......... .......... 27% 18.8M 0s\n  1500K .......... .......... .......... .......... .......... 28% 29.7M 0s\n  1550K .......... .......... .......... .......... .......... 29% 30.4M 0s\n  1600K .......... .......... .......... .......... .......... 30%  142M 0s\n  1650K .......... .......... .......... .......... .......... 31% 11.0M 0s\n  1700K .......... .......... .......... .......... .......... 32% 30.4M 0s\n  1750K .......... .......... .......... .......... .......... 32% 21.3M 0s\n  1800K .......... .......... .......... .......... .......... 33% 20.5M 0s\n  1850K .......... .......... .......... .......... .......... 34%  141M 0s\n  1900K .......... .......... .......... .......... .......... 35% 20.1M 0s\n  1950K .......... .......... .......... .......... .......... 36%  109M 0s\n  2000K .......... .......... .......... .......... .......... 37%  149M 0s\n  2050K .......... .......... .......... .......... .......... 38% 9.11M 0s\n  2100K .......... .......... .......... .......... .......... 39% 31.1M 0s\n  2150K .......... .......... .......... .......... .......... 40% 18.7M 0s\n  2200K .......... .......... .......... .......... .......... 41% 20.3M 0s\n  2250K .......... .......... .......... .......... .......... 42% 48.5M 0s\n  2300K .......... .......... .......... .......... .......... 43%  130M 0s\n  2350K .......... .......... .......... .......... .......... 43% 27.1M 0s\n  2400K .......... .......... .......... .......... .......... 44%  145M 0s\n  2450K .......... .......... .......... .......... .......... 45% 23.9M 0s\n  2500K .......... .......... .......... .......... .......... 46% 39.9M 0s\n  2550K .......... .......... .......... .......... .......... 47% 21.0M 0s\n  2600K .......... .......... .......... .......... .......... 48%  114M 0s\n  2650K .......... .......... .......... .......... .......... 49%  136M 0s\n  2700K .......... .......... .......... .......... .......... 50% 43.2M 0s\n  2750K .......... .......... .......... .......... .......... 51% 42.0M 0s\n  2800K .......... .......... .......... .......... .......... 52%  106M 0s\n  2850K .......... .......... .......... .......... .......... 53%  139M 0s\n  2900K .......... .......... .......... .......... .......... 54% 98.4M 0s\n  2950K .......... .......... .......... .......... .......... 54% 39.0M 0s\n  3000K .......... .......... .......... .......... .......... 55%  142M 0s\n  3050K .......... .......... .......... .......... .......... 56%  143M 0s\n  3100K .......... .......... .......... .......... .......... 57% 52.4M 0s\n  3150K .......... .......... .......... .......... .......... 58% 86.5M 0s\n  3200K .......... .......... .......... .......... .......... 59%  133M 0s\n  3250K .......... .......... .......... .......... .......... 60%  137M 0s\n  3300K .......... .......... .......... .......... .......... 61%  105M 0s\n  3350K .......... .......... .......... .......... .......... 62%  124M 0s\n  3400K .......... .......... .......... .......... .......... 63%  107M 0s\n  3450K .......... .......... .......... .......... .......... 64% 77.3M 0s\n  3500K .......... .......... .......... .......... .......... 65% 93.3M 0s\n  3550K .......... .......... .......... .......... .......... 65% 32.9M 0s\n  3600K .......... .......... .......... .......... .......... 66%  147M 0s\n  3650K .......... .......... .......... .......... .......... 67%  144M 0s\n  3700K .......... .......... .......... .......... .......... 68% 86.4M 0s\n  3750K .......... .......... .......... .......... .......... 69% 38.5M 0s\n  3800K .......... .......... .......... .......... .......... 70%  111M 0s\n  3850K .......... .......... .......... .......... .......... 71% 6.36M 0s\n  3900K .......... .......... .......... .......... .......... 72% 35.5M 0s\n  3950K .......... .......... .......... .......... .......... 73% 85.3M 0s\n  4000K .......... .......... .......... .......... .......... 74%  126M 0s\n  4050K .......... .......... .......... .......... .......... 75% 82.6M 0s\n  4100K .......... .......... .......... .......... .......... 76%  135M 0s\n  4150K .......... .......... .......... .......... .......... 76% 97.8M 0s\n  4200K .......... .......... .......... .......... .......... 77%  132M 0s\n  4250K .......... .......... .......... .......... .......... 78%  138M 0s\n  4300K .......... .......... .......... .......... .......... 79%  122M 0s\n  4350K .......... .......... .......... .......... .......... 80%  125M 0s\n  4400K .......... .......... .......... .......... .......... 81% 77.4M 0s\n  4450K .......... .......... .......... .......... .......... 82%  140M 0s\n  4500K .......... .......... .......... .......... .......... 83%  119M 0s\n  4550K .......... .......... .......... .......... .......... 84%  100M 0s\n  4600K .......... .......... .......... .......... .......... 85%  142M 0s\n  4650K .......... .......... .......... .......... .......... 86% 52.7M 0s\n  4700K .......... .......... .......... .......... .......... 87%  130M 0s\n  4750K .......... .......... .......... .......... .......... 87%  103M 0s\n  4800K .......... .......... .......... .......... .......... 88%  149M 0s\n  4850K .......... .......... .......... .......... .......... 89% 83.9M 0s\n  4900K .......... .......... .......... .......... .......... 90%  121M 0s\n  4950K .......... .......... .......... .......... .......... 91%  114M 0s\n  5000K .......... .......... .......... .......... .......... 92% 53.0M 0s\n  5050K .......... .......... .......... .......... .......... 93%  137M 0s\n  5100K .......... .......... .......... .......... .......... 94% 81.3M 0s\n  5150K .......... .......... .......... .......... .......... 95%  122M 0s\n  5200K .......... .......... .......... .......... .......... 96%  108M 0s\n  5250K .......... .......... .......... .......... .......... 97%  135M 0s\n  5300K .......... .......... .......... .......... .......... 98%  131M 0s\n  5350K .......... .......... .......... .......... .......... 98% 93.6M 0s\n  5400K .......... .......... .......... .......... .......... 99% 91.5M 0s\n  5450K ........                                              100%  151M=0.2s\n\n2022-01-16 20:28:13 (32.4 MB/s) - ‘shakespeare.txt’ saved [5589889/5589889]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["--2022-01-16 20:28:13--  https://waterponey.github.io/BigDataClass/files/shakespeare.txt\nResolving waterponey.github.io (waterponey.github.io)... 185.199.111.153, 185.199.108.153, 185.199.109.153, ...\nConnecting to waterponey.github.io (waterponey.github.io)|185.199.111.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5589889 (5.3M) [text/plain]\nSaving to: ‘shakespeare.txt’\n\n     0K .......... .......... .......... .......... ..........  0% 5.03M 1s\n    50K .......... .......... .......... .......... ..........  1% 5.16M 1s\n   100K .......... .......... .......... .......... ..........  2% 5.45M 1s\n   150K .......... .......... .......... .......... ..........  3% 17.3M 1s\n   200K .......... .......... .......... .......... ..........  4% 26.9M 1s\n   250K .......... .......... .......... .......... ..........  5% 11.2M 1s\n   300K .......... .......... .......... .......... ..........  6% 64.5M 1s\n   350K .......... .......... .......... .......... ..........  7% 18.6M 1s\n   400K .......... .......... .......... .......... ..........  8% 60.1M 0s\n   450K .......... .......... .......... .......... ..........  9% 51.5M 0s\n   500K .......... .......... .......... .......... .......... 10% 10.2M 0s\n   550K .......... .......... .......... .......... .......... 10% 73.6M 0s\n   600K .......... .......... .......... .......... .......... 11%  153M 0s\n   650K .......... .......... .......... .......... .......... 12% 31.2M 0s\n   700K .......... .......... .......... .......... .......... 13% 35.1M 0s\n   750K .......... .......... .......... .......... .......... 14% 5.35M 0s\n   800K .......... .......... .......... .......... .......... 15% 15.0M 0s\n   850K .......... .......... .......... .......... .......... 16%  138M 0s\n   900K .......... .......... .......... .......... .......... 17% 18.8M 0s\n   950K .......... .......... .......... .......... .......... 18% 41.6M 0s\n  1000K .......... .......... .......... .......... .......... 19%  130M 0s\n  1050K .......... .......... .......... .......... .......... 20%  145M 0s\n  1100K .......... .......... .......... .......... .......... 21% 10.1M 0s\n  1150K .......... .......... .......... .......... .......... 21% 30.9M 0s\n  1200K .......... .......... .......... .......... .......... 22% 15.9M 0s\n  1250K .......... .......... .......... .......... .......... 23% 23.3M 0s\n  1300K .......... .......... .......... .......... .......... 24%  132M 0s\n  1350K .......... .......... .......... .......... .......... 25% 18.0M 0s\n  1400K .......... .......... .......... .......... .......... 26% 32.1M 0s\n  1450K .......... .......... .......... .......... .......... 27% 18.8M 0s\n  1500K .......... .......... .......... .......... .......... 28% 29.7M 0s\n  1550K .......... .......... .......... .......... .......... 29% 30.4M 0s\n  1600K .......... .......... .......... .......... .......... 30%  142M 0s\n  1650K .......... .......... .......... .......... .......... 31% 11.0M 0s\n  1700K .......... .......... .......... .......... .......... 32% 30.4M 0s\n  1750K .......... .......... .......... .......... .......... 32% 21.3M 0s\n  1800K .......... .......... .......... .......... .......... 33% 20.5M 0s\n  1850K .......... .......... .......... .......... .......... 34%  141M 0s\n  1900K .......... .......... .......... .......... .......... 35% 20.1M 0s\n  1950K .......... .......... .......... .......... .......... 36%  109M 0s\n  2000K .......... .......... .......... .......... .......... 37%  149M 0s\n  2050K .......... .......... .......... .......... .......... 38% 9.11M 0s\n  2100K .......... .......... .......... .......... .......... 39% 31.1M 0s\n  2150K .......... .......... .......... .......... .......... 40% 18.7M 0s\n  2200K .......... .......... .......... .......... .......... 41% 20.3M 0s\n  2250K .......... .......... .......... .......... .......... 42% 48.5M 0s\n  2300K .......... .......... .......... .......... .......... 43%  130M 0s\n  2350K .......... .......... .......... .......... .......... 43% 27.1M 0s\n  2400K .......... .......... .......... .......... .......... 44%  145M 0s\n  2450K .......... .......... .......... .......... .......... 45% 23.9M 0s\n  2500K .......... .......... .......... .......... .......... 46% 39.9M 0s\n  2550K .......... .......... .......... .......... .......... 47% 21.0M 0s\n  2600K .......... .......... .......... .......... .......... 48%  114M 0s\n  2650K .......... .......... .......... .......... .......... 49%  136M 0s\n  2700K .......... .......... .......... .......... .......... 50% 43.2M 0s\n  2750K .......... .......... .......... .......... .......... 51% 42.0M 0s\n  2800K .......... .......... .......... .......... .......... 52%  106M 0s\n  2850K .......... .......... .......... .......... .......... 53%  139M 0s\n  2900K .......... .......... .......... .......... .......... 54% 98.4M 0s\n  2950K .......... .......... .......... .......... .......... 54% 39.0M 0s\n  3000K .......... .......... .......... .......... .......... 55%  142M 0s\n  3050K .......... .......... .......... .......... .......... 56%  143M 0s\n  3100K .......... .......... .......... .......... .......... 57% 52.4M 0s\n  3150K .......... .......... .......... .......... .......... 58% 86.5M 0s\n  3200K .......... .......... .......... .......... .......... 59%  133M 0s\n  3250K .......... .......... .......... .......... .......... 60%  137M 0s\n  3300K .......... .......... .......... .......... .......... 61%  105M 0s\n  3350K .......... .......... .......... .......... .......... 62%  124M 0s\n  3400K .......... .......... .......... .......... .......... 63%  107M 0s\n  3450K .......... .......... .......... .......... .......... 64% 77.3M 0s\n  3500K .......... .......... .......... .......... .......... 65% 93.3M 0s\n  3550K .......... .......... .......... .......... .......... 65% 32.9M 0s\n  3600K .......... .......... .......... .......... .......... 66%  147M 0s\n  3650K .......... .......... .......... .......... .......... 67%  144M 0s\n  3700K .......... .......... .......... .......... .......... 68% 86.4M 0s\n  3750K .......... .......... .......... .......... .......... 69% 38.5M 0s\n  3800K .......... .......... .......... .......... .......... 70%  111M 0s\n  3850K .......... .......... .......... .......... .......... 71% 6.36M 0s\n  3900K .......... .......... .......... .......... .......... 72% 35.5M 0s\n  3950K .......... .......... .......... .......... .......... 73% 85.3M 0s\n  4000K .......... .......... .......... .......... .......... 74%  126M 0s\n  4050K .......... .......... .......... .......... .......... 75% 82.6M 0s\n  4100K .......... .......... .......... .......... .......... 76%  135M 0s\n  4150K .......... .......... .......... .......... .......... 76% 97.8M 0s\n  4200K .......... .......... .......... .......... .......... 77%  132M 0s\n  4250K .......... .......... .......... .......... .......... 78%  138M 0s\n  4300K .......... .......... .......... .......... .......... 79%  122M 0s\n  4350K .......... .......... .......... .......... .......... 80%  125M 0s\n  4400K .......... .......... .......... .......... .......... 81% 77.4M 0s\n  4450K .......... .......... .......... .......... .......... 82%  140M 0s\n  4500K .......... .......... .......... .......... .......... 83%  119M 0s\n  4550K .......... .......... .......... .......... .......... 84%  100M 0s\n  4600K .......... .......... .......... .......... .......... 85%  142M 0s\n  4650K .......... .......... .......... .......... .......... 86% 52.7M 0s\n  4700K .......... .......... .......... .......... .......... 87%  130M 0s\n  4750K .......... .......... .......... .......... .......... 87%  103M 0s\n  4800K .......... .......... .......... .......... .......... 88%  149M 0s\n  4850K .......... .......... .......... .......... .......... 89% 83.9M 0s\n  4900K .......... .......... .......... .......... .......... 90%  121M 0s\n  4950K .......... .......... .......... .......... .......... 91%  114M 0s\n  5000K .......... .......... .......... .......... .......... 92% 53.0M 0s\n  5050K .......... .......... .......... .......... .......... 93%  137M 0s\n  5100K .......... .......... .......... .......... .......... 94% 81.3M 0s\n  5150K .......... .......... .......... .......... .......... 95%  122M 0s\n  5200K .......... .......... .......... .......... .......... 96%  108M 0s\n  5250K .......... .......... .......... .......... .......... 97%  135M 0s\n  5300K .......... .......... .......... .......... .......... 98%  131M 0s\n  5350K .......... .......... .......... .......... .......... 98% 93.6M 0s\n  5400K .......... .......... .......... .......... .......... 99% 91.5M 0s\n  5450K ........                                              100%  151M=0.2s\n\n2022-01-16 20:28:13 (32.4 MB/s) - ‘shakespeare.txt’ saved [5589889/5589889]\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["file_path = 'file:/databricks/driver/shakespeare.txt'\n\nshakespeare_RDD = (sc.textFile(file_path, 8)\n                     .map(remove_punctuation))\n\nprint('\\n'.join(shakespeare_RDD\n                .zipWithIndex()  # to (line, lineNum) pairRDD\n                .map(lambda pair: '{0}: {1}'.format(pair[1], pair[0]))  # to 'lineNum: line'\n                .take(15)))"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0084340c-58b6-4333-86b5-982d9c631995"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"0: the project gutenberg ebook of the complete works of william shakespeare by\n1: william shakespeare\n2: \n3: this ebook is for the use of anyone anywhere at no cost and with\n4: almost no restrictions whatsoever  you may copy it give it away or\n5: reuse it under the terms of the project gutenberg license included\n6: with this ebook or online at wwwgutenbergorg\n7: \n8: this is a copyrighted project gutenberg ebook details below\n9: please follow the copyright guidelines in this file\n10: \n11: title the complete works of william shakespeare\n12: \n13: author william shakespeare\n14: \n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["0: the project gutenberg ebook of the complete works of william shakespeare by\n1: william shakespeare\n2: \n3: this ebook is for the use of anyone anywhere at no cost and with\n4: almost no restrictions whatsoever  you may copy it give it away or\n5: reuse it under the terms of the project gutenberg license included\n6: with this ebook or online at wwwgutenbergorg\n7: \n8: this is a copyrighted project gutenberg ebook details below\n9: please follow the copyright guidelines in this file\n10: \n11: title the complete works of william shakespeare\n12: \n13: author william shakespeare\n14: \n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Before we can use the `word_count()` function, we have to address two issues with the format of the RDD:\n  + #### The first issue is that  that we need to split each line by its spaces.\n  + #### The second issue is we need to filter out empty lines.\n \nApply a transformation that will split each element of the RDD by its spaces. For each element of the RDD, you should apply Python's string [split()](https://docs.python.org/2/library/string.html#string.split) function. You might think that a [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) transformation is the way to do this, but think about what the result of the `split()` function will be: there is a better option.\n\nHint: remember the problem we had with `GroupByKey()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"668ddad0-7743-4bfc-9065-86173b56106a"}}},{"cell_type":"code","source":["shakespeare_words_RDD = shakespeare_RDD.flatMap(lambda x: x.split(\" \"))\nshakespeare_word_count_elem = shakespeare_words_RDD.count()\n\nprint(shakespeare_words_RDD.take(5))\nprint(shakespeare_word_count_elem)\n\nassert shakespeare_word_count_elem == 1010679, \\\n       'incorrect value for shakespeare_word_count_elem'\n\nassert hash(shakespeare_words_RDD.top(5)) == '036f886dad6af36a651fd51910180a9a0e267d84', \\\n       'incorrect value for shakespeare_words_RDD'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f3341be-67c9-4b96-8686-92f5582d3712"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['the', 'project', 'gutenberg', 'ebook', 'of']\n949075\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['the', 'project', 'gutenberg', 'ebook', 'of']\n949075\n"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-4435601902767710>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshakespeare_word_count_elem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0;32massert\u001B[0m \u001B[0mshakespeare_word_count_elem\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1010679\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m        \u001B[0;34m'incorrect value for shakespeare_word_count_elem'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAssertionError\u001B[0m: incorrect value for shakespeare_word_count_elem","errorSummary":"<span class='ansi-red-fg'>AssertionError</span>: incorrect value for shakespeare_word_count_elem","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-4435601902767710>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshakespeare_word_count_elem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0;32massert\u001B[0m \u001B[0mshakespeare_word_count_elem\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1010679\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m        \u001B[0;34m'incorrect value for shakespeare_word_count_elem'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAssertionError\u001B[0m: incorrect value for shakespeare_word_count_elem"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["The next step is to filter out the empty elements.  Remove all entries where the word is `''`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"295da30d-2e15-4211-91c4-e6373b4740ef"}}},{"cell_type":"code","source":["shakespeare_nonempty_words_RDD = shakespeare_words_RDD.filter(lambda x: len(x)>0)\nshakespeare_nonempty_word_elem_count = shakespeare_nonempty_words_RDD.count()\n\nprint(shakespeare_nonempty_words_RDD.take(20))\nprint(shakespeare_nonempty_word_elem_count)\n\nassert shakespeare_nonempty_word_elem_count == 960028, \\\n       'incorrect value for shakespeare_nonempty_word_elem_count'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cf0eb6e-37a9-4091-ab99-5559f302ea73"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'by', 'william', 'shakespeare', 'this', 'ebook', 'is', 'for', 'the', 'use']\n903707\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'by', 'william', 'shakespeare', 'this', 'ebook', 'is', 'for', 'the', 'use']\n903707\n"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-4435601902767712>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshakespeare_nonempty_word_elem_count\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0;32massert\u001B[0m \u001B[0mshakespeare_nonempty_word_elem_count\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m960028\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m        \u001B[0;34m'incorrect value for shakespeare_nonempty_word_elem_count'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAssertionError\u001B[0m: incorrect value for shakespeare_nonempty_word_elem_count","errorSummary":"<span class='ansi-red-fg'>AssertionError</span>: incorrect value for shakespeare_nonempty_word_elem_count","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-4435601902767712>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshakespeare_nonempty_word_elem_count\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0;32massert\u001B[0m \u001B[0mshakespeare_nonempty_word_elem_count\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m960028\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m        \u001B[0;34m'incorrect value for shakespeare_nonempty_word_elem_count'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAssertionError\u001B[0m: incorrect value for shakespeare_nonempty_word_elem_count"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["You now have an RDD that contains only words.  Next, apply the `word_count()` function to produce a list of word counts. We can view the top 15 words by using the `takeOrdered()` action. However, since the elements of the RDD are pairs, you will need a custom sort function that sorts using the value part of the pair.\n\nUse the `wordCount()` function and `takeOrdered()` to obtain the fifteen most common words and their counts."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd2768cb-ef22-46ef-bbec-3190aff02c0b"}}},{"cell_type":"code","source":["top15_words = word_count(shakespeare_nonempty_words_RDD).takeOrdered(15, key=lambda x: -x[1])\n\nprint('\\n'.join(map(lambda pair: '{0}: {1}'.format(pair[0], pair[1]), top15_words)))\n\nassert hash(top15_words) == '3c4a0974cc09536ba8902683e0ad441d8239a710', \\\n       'incorrect value for top15WordsAndCounts'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc632625-f2fb-4133-ac19-fac307eecb88"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"the: 27825\nand: 26791\ni: 20681\nto: 19261\nof: 18289\na: 14667\nyou: 13716\nmy: 12481\nthat: 11135\nin: 11027\nis: 9621\nnot: 8745\nfor: 8261\nwith: 8046\nme: 7769\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["the: 27825\nand: 26791\ni: 20681\nto: 19261\nof: 18289\na: 14667\nyou: 13716\nmy: 12481\nthat: 11135\nin: 11027\nis: 9621\nnot: 8745\nfor: 8261\nwith: 8046\nme: 7769\n"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-4435601902767714>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'\\n'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mpair\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'{0}: {1}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpair\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpair\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtop15_words\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;32massert\u001B[0m \u001B[0mhash\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtop15_words\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'3c4a0974cc09536ba8902683e0ad441d8239a710'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m        \u001B[0;34m'incorrect value for top15WordsAndCounts'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAssertionError\u001B[0m: incorrect value for top15WordsAndCounts","errorSummary":"<span class='ansi-red-fg'>AssertionError</span>: incorrect value for top15WordsAndCounts","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-4435601902767714>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'\\n'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mpair\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'{0}: {1}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpair\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpair\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtop15_words\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;32massert\u001B[0m \u001B[0mhash\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtop15_words\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'3c4a0974cc09536ba8902683e0ad441d8239a710'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m        \u001B[0;34m'incorrect value for top15WordsAndCounts'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAssertionError\u001B[0m: incorrect value for top15WordsAndCounts"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["You will notice that many of the words are common English words. These are called stopwords. In practice, when we do natural language processing, we filter these stopwords as they do not contain a lot of information."],"metadata":{"collapsed":true,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28d80e77-e510-4d13-ba8c-a86571670698"}}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.9","nbconvert_exporter":"python","file_extension":".py"},"name":"2-spark-rdd","notebookId":1361410605051095,"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"application/vnd.databricks.v1+notebook":{"notebookName":"3-spark-rdd","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4435601902767675}},"nbformat":4,"nbformat_minor":0}
